test compile precise-output
set unwind_info=false
target aarch64

function %a(i8) -> i8 {
block0(v0: i8):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit w3, w0
;   lsr w0, w3, #24
;   ret

function %a(i16) -> i16 {
block0(v0: i16):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit w3, w0
;   lsr w0, w3, #16
;   ret

function %a(i32) -> i32 {
block0(v0: i32):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit w0, w0
;   ret

function %a(i64) -> i64 {
block0(v0: i64):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit x0, x0
;   ret

function %a(i128) -> i128 {
block0(v0: i128):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit x5, x0
;   rbit x0, x1
;   mov x1, x5
;   ret

function %b(i8) -> i8 {
block0(v0: i8):
    v1 = clz v0
    return v1
}

; block0:
;   uxtb w3, w0
;   clz w5, w3
;   sub w0, w5, #24
;   ret

function %b(i16) -> i16 {
block0(v0: i16):
    v1 = clz v0
    return v1
}

; block0:
;   uxth w3, w0
;   clz w5, w3
;   sub w0, w5, #16
;   ret

function %b(i32) -> i32 {
block0(v0: i32):
    v1 = clz v0
    return v1
}

; block0:
;   clz w0, w0
;   ret

function %b(i64) -> i64 {
block0(v0: i64):
    v1 = clz v0
    return v1
}

; block0:
;   clz x0, x0
;   ret

function %b(i128) -> i128 {
block0(v0: i128):
    v1 = clz v0
    return v1
}

; block0:
;   clz x5, x1
;   clz x7, x0
;   lsr x9, x5, #6
;   madd x0, x7, x9, x5
;   movz w1, #0
;   ret

function %c(i8) -> i8 {
block0(v0: i8):
    v1 = cls v0
    return v1
}

; block0:
;   sxtb w3, w0
;   cls w5, w3
;   sub w0, w5, #24
;   ret

function %c(i16) -> i16 {
block0(v0: i16):
    v1 = cls v0
    return v1
}

; block0:
;   sxth w3, w0
;   cls w5, w3
;   sub w0, w5, #16
;   ret

function %c(i32) -> i32 {
block0(v0: i32):
    v1 = cls v0
    return v1
}

; block0:
;   cls w0, w0
;   ret

function %c(i64) -> i64 {
block0(v0: i64):
    v1 = cls v0
    return v1
}

; block0:
;   cls x0, x0
;   ret

function %c(i128) -> i128 {
block0(v0: i128):
    v1 = cls v0
    return v1
}

; block0:
;   cls x5, x0
;   cls x7, x1
;   eon x9, x1, x0
;   lsr x11, x9, #63
;   madd x13, x5, x11, x11
;   subs xzr, x7, #63
;   csel x0, x13, xzr, eq
;   add x0, x0, x7
;   movz w1, #0
;   ret

function %d(i8) -> i8 {
block0(v0: i8):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit w3, w0
;   orr w5, w3, #8388608
;   clz w0, w5
;   ret

function %d(i16) -> i16 {
block0(v0: i16):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit w3, w0
;   orr w5, w3, #32768
;   clz w0, w5
;   ret

function %d(i32) -> i32 {
block0(v0: i32):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit w3, w0
;   clz w0, w3
;   ret

function %d(i64) -> i64 {
block0(v0: i64):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit x3, x0
;   clz x0, x3
;   ret

function %d(i128) -> i128 {
block0(v0: i128):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit x5, x0
;   rbit x7, x1
;   clz x9, x5
;   clz x11, x7
;   lsr x13, x9, #6
;   madd x0, x11, x13, x9
;   movz w1, #0
;   ret

function %d(i128) -> i128 {
block0(v0: i128):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov d6, x0
;   mov v6.d[1], v6.d[1], x1
;   cnt v17.16b, v6.16b
;   addv b19, v17.16b
;   umov w0, v19.b[0]
;   movz w1, #0
;   ret

function %d(i64) -> i64 {
block0(v0: i64):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov d3, x0
;   cnt v5.8b, v3.8b
;   addv b7, v5.8b
;   umov w0, v7.b[0]
;   ret

function %d(i32) -> i32 {
block0(v0: i32):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov s3, w0
;   cnt v5.8b, v3.8b
;   addv b7, v5.8b
;   umov w0, v7.b[0]
;   ret

function %d(i16) -> i16 {
block0(v0: i16):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov s3, w0
;   cnt v5.8b, v3.8b
;   addp v7.8b, v5.8b, v5.8b
;   umov w0, v7.b[0]
;   ret

function %d(i8) -> i8 {
block0(v0: i8):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov s3, w0
;   cnt v5.8b, v3.8b
;   umov w0, v5.b[0]
;   ret

function %bextend_b8() -> b32 {
block0:
    v1 = bconst.b8 true
    v2 = bextend.b32 v1
    return v2
}

; block0:
;   movz x1, #255
;   sxtb w0, w1
;   ret

function %bextend_b1() -> b32 {
block0:
    v1 = bconst.b1 true
    v2 = bextend.b32 v1
    return v2
}

; block0:
;   movz x1, #1
;   sbfx w0, w1, #0, #1
;   ret

function %bnot_i32(i32) -> i32 {
block0(v0: i32):
    v1 = bnot v0
    return v1
}

; block0:
;   orn w0, wzr, w0
;   ret

function %bnot_i64(i64) -> i64 {
block0(v0: i64):
    v1 = bnot v0
    return v1
}

; block0:
;   orn x0, xzr, x0
;   ret

function %bnot_i64_with_shift(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = ishl.i64 v0, v1
    v3 = bnot v2
    return v3
}

; block0:
;   orn x0, xzr, x0, LSL 3
;   ret

function %bnot_i128(i128) -> i128 {
block0(v0: i128):
    v1 = bnot v0
    return v1
}

; block0:
;   orn x0, xzr, x0
;   orn x1, xzr, x1
;   ret

function %bnot_i8x16(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = bnot v0
    return v1
}

; block0:
;   mvn v0.16b, v0.16b
;   ret

function %band_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v0, v1
    return v2
}

; block0:
;   and w0, w0, w1
;   ret

function %band_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band v0, v1
    return v2
}

; block0:
;   and x0, x0, x1
;   ret

function %band_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = band v0, v1
    return v2
}

; block0:
;   and x0, x0, x2
;   and x1, x1, x3
;   ret

function %band_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = band v0, v1
    return v2
}

; block0:
;   and v0.16b, v0.16b, v1.16b
;   ret

function %band_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = band v0, v1
    return v2
}

; block0:
;   and x0, x0, #3
;   ret

function %band_i64_constant2(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = band v1, v0
    return v2
}

; block0:
;   and x0, x0, #3
;   ret

function %band_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = band v0, v3
    return v4
}

; block0:
;   and x0, x0, x1, LSL 3
;   ret

function %band_i64_constant_shift2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = band v3, v0
    return v4
}

; block0:
;   and x0, x0, x1, LSL 3
;   ret

function %bor_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr w0, w0, w1
;   ret

function %bor_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr x0, x0, x1
;   ret

function %bor_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr x0, x0, x2
;   orr x1, x1, x3
;   ret

function %bor_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr v0.16b, v0.16b, v1.16b
;   ret

function %bor_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr x0, x0, #3
;   ret

function %bor_i64_constant2(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bor v1, v0
    return v2
}

; block0:
;   orr x0, x0, #3
;   ret

function %bor_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bor v0, v3
    return v4
}

; block0:
;   orr x0, x0, x1, LSL 3
;   ret

function %bor_i64_constant_shift2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bor v3, v0
    return v4
}

; block0:
;   orr x0, x0, x1, LSL 3
;   ret

function %bxor_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor w0, w0, w1
;   ret

function %bxor_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor x0, x0, x1
;   ret

function %bxor_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor x0, x0, x2
;   eor x1, x1, x3
;   ret

function %bxor_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor v0.16b, v0.16b, v1.16b
;   ret

function %bxor_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor x0, x0, #3
;   ret

function %bxor_i64_constant2(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bxor v1, v0
    return v2
}

; block0:
;   eor x0, x0, #3
;   ret

function %bxor_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bxor v0, v3
    return v4
}

; block0:
;   eor x0, x0, x1, LSL 3
;   ret

function %bxor_i64_constant_shift2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bxor v3, v0
    return v4
}

; block0:
;   eor x0, x0, x1, LSL 3
;   ret

function %band_not_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic w0, w0, w1
;   ret

function %band_not_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic x0, x0, x1
;   ret

function %band_not_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic x0, x0, x2
;   bic x1, x1, x3
;   ret

function %band_not_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic v0.16b, v0.16b, v1.16b
;   ret

function %band_not_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 4
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic x0, x0, #4
;   ret

function %band_not_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 4
    v3 = ishl.i64 v1, v2
    v4 = band_not v0, v3
    return v4
}

; block0:
;   bic x0, x0, x1, LSL 4
;   ret

function %bor_not_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn w0, w0, w1
;   ret

function %bor_not_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn x0, x0, x1
;   ret

function %bor_not_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn x0, x0, x2
;   orn x1, x1, x3
;   ret

function %bor_not_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 4
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn x0, x0, #4
;   ret

function %bor_not_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 4
    v3 = ishl.i64 v1, v2
    v4 = bor_not v0, v3
    return v4
}

; block0:
;   orn x0, x0, x1, LSL 4
;   ret

function %bxor_not_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon w0, w0, w1
;   ret

function %bxor_not_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon x0, x0, x1
;   ret

function %bxor_not_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon x0, x0, x2
;   eon x1, x1, x3
;   ret

function %bxor_not_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 4
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon x0, x0, #4
;   ret

function %bxor_not_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 4
    v3 = ishl.i64 v1, v2
    v4 = bxor_not v0, v3
    return v4
}

; block0:
;   eon x0, x0, x1, LSL 4
;   ret

function %ishl_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = ishl.i128 v0, v1
    return v2
}

; block0:
;   lsl x6, x0, x2
;   lsl x8, x1, x2
;   orn w10, wzr, w2
;   lsr x12, x0, #1
;   lsr x14, x12, x10
;   orr x1, x8, x14
;   ands xzr, x2, #64
;   csel x0, xzr, x6, ne
;   csel x1, x6, x1, ne
;   ret

function %ishl_i128_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = ishl.i128 v0, v1
    return v2
}

; block0:
;   lsl x7, x0, x2
;   lsl x9, x1, x2
;   orn w11, wzr, w2
;   lsr x13, x0, #1
;   lsr x15, x13, x11
;   orr x1, x9, x15
;   ands xzr, x2, #64
;   csel x0, xzr, x7, ne
;   csel x1, x7, x1, ne
;   ret

function %ushr_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = ushr.i128 v0, v1
    return v2
}

; block0:
;   lsr x6, x0, x2
;   lsr x8, x1, x2
;   orn w10, wzr, w2
;   lsl x12, x1, #1
;   lsl x14, x12, x10
;   orr x0, x6, x14
;   ands xzr, x2, #64
;   csel x0, x8, x0, ne
;   csel x1, xzr, x8, ne
;   ret

function %ushr_i128_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = ushr.i128 v0, v1
    return v2
}

; block0:
;   lsr x7, x0, x2
;   lsr x9, x1, x2
;   orn w11, wzr, w2
;   lsl x13, x1, #1
;   lsl x15, x13, x11
;   orr x1, x7, x15
;   ands xzr, x2, #64
;   csel x0, x9, x1, ne
;   csel x1, xzr, x9, ne
;   ret

function %sshr_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = sshr.i128 v0, v1
    return v2
}

; block0:
;   lsr x6, x0, x2
;   asr x8, x1, x2
;   orn w10, wzr, w2
;   lsl x12, x1, #1
;   lsl x14, x12, x10
;   asr x1, x1, #63
;   orr x3, x6, x14
;   ands xzr, x2, #64
;   csel x0, x8, x3, ne
;   csel x1, x1, x8, ne
;   ret

function %sshr_i128_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = sshr.i128 v0, v1
    return v2
}

; block0:
;   lsr x7, x0, x2
;   asr x9, x1, x2
;   orn w11, wzr, w2
;   lsl x13, x1, #1
;   lsl x15, x13, x11
;   asr x1, x1, #63
;   orr x3, x7, x15
;   ands xzr, x2, #64
;   csel x0, x9, x3, ne
;   csel x1, x1, x9, ne
;   ret

