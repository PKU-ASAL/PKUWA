test compile precise-output
set unwind_info=false
target aarch64

function %foo() {
block0:
    return
}

; block0:
;   ret

function %stack_limit_leaf_zero(i64 stack_limit) {
block0(v0: i64):
    return
}

; block0:
;   ret

function %stack_limit_gv_leaf_zero(i64 vmctx) {
    gv0 = vmctx
    gv1 = load.i64 notrap aligned gv0
    gv2 = load.i64 notrap aligned gv1+4
    stack_limit = gv2
block0(v0: i64):
    return
}

; block0:
;   ret

function %stack_limit_call_zero(i64 stack_limit) {
    fn0 = %foo()
block0(v0: i64):
    call fn0()
    return
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   subs xzr, sp, x0, UXTX
;   b.hs 8 ; udf
; block0:
;   ldr x2, 8 ; b 12 ; data TestCase(%foo) + 0
;   blr x2
;   ldp fp, lr, [sp], #16
;   ret

function %stack_limit_gv_call_zero(i64 vmctx) {
    gv0 = vmctx
    gv1 = load.i64 notrap aligned gv0
    gv2 = load.i64 notrap aligned gv1+4
    stack_limit = gv2
    fn0 = %foo()
block0(v0: i64):
    call fn0()
    return
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   ldr x16, [x0]
;   ldr x16, [x16, #4]
;   subs xzr, sp, x16, UXTX
;   b.hs 8 ; udf
; block0:
;   ldr x2, 8 ; b 12 ; data TestCase(%foo) + 0
;   blr x2
;   ldp fp, lr, [sp], #16
;   ret

function %stack_limit(i64 stack_limit) {
    ss0 = explicit_slot 168
block0(v0: i64):
    return
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   add x16, x0, #176
;   subs xzr, sp, x16, UXTX
;   b.hs 8 ; udf
;   sub sp, sp, #176
; block0:
;   add sp, sp, #176
;   ldp fp, lr, [sp], #16
;   ret

function %huge_stack_limit(i64 stack_limit) {
    ss0 = explicit_slot 400000
block0(v0: i64):
    return
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   subs xzr, sp, x0, UXTX
;   b.hs 8 ; udf
;   movz w17, #6784
;   movk w17, w17, #6, LSL #16
;   add x16, x0, x17, UXTX
;   subs xzr, sp, x16, UXTX
;   b.hs 8 ; udf
;   movz w16, #6784
;   movk w16, w16, #6, LSL #16
;   sub sp, sp, x16, UXTX
; block0:
;   movz w16, #6784
;   movk w16, w16, #6, LSL #16
;   add sp, sp, x16, UXTX
;   ldp fp, lr, [sp], #16
;   ret

function %limit_preamble(i64 vmctx) {
    gv0 = vmctx
    gv1 = load.i64 notrap aligned gv0
    gv2 = load.i64 notrap aligned gv1+4
    stack_limit = gv2
    ss0 = explicit_slot 20
block0(v0: i64):
    return
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   ldr x16, [x0]
;   ldr x16, [x16, #4]
;   add x16, x16, #32
;   subs xzr, sp, x16, UXTX
;   b.hs 8 ; udf
;   sub sp, sp, #32
; block0:
;   add sp, sp, #32
;   ldp fp, lr, [sp], #16
;   ret

function %limit_preamble_huge(i64 vmctx) {
    gv0 = vmctx
    gv1 = load.i64 notrap aligned gv0
    gv2 = load.i64 notrap aligned gv1+4
    stack_limit = gv2
    ss0 = explicit_slot 400000
block0(v0: i64):
    return
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   ldr x16, [x0]
;   ldr x16, [x16, #4]
;   subs xzr, sp, x16, UXTX
;   b.hs 8 ; udf
;   movz w17, #6784
;   movk w17, w17, #6, LSL #16
;   add x16, x16, x17, UXTX
;   subs xzr, sp, x16, UXTX
;   b.hs 8 ; udf
;   movz w16, #6784
;   movk w16, w16, #6, LSL #16
;   sub sp, sp, x16, UXTX
; block0:
;   movz w16, #6784
;   movk w16, w16, #6, LSL #16
;   add sp, sp, x16, UXTX
;   ldp fp, lr, [sp], #16
;   ret

function %limit_preamble_huge_offset(i64 vmctx) {
    gv0 = vmctx
    gv1 = load.i64 notrap aligned gv0+400000
    stack_limit = gv1
    ss0 = explicit_slot 20
block0(v0: i64):
    return
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   movz w16, #6784 ; movk w16, w16, #6, LSL #16 ; add x16, x0, x16, UXTX ; ldr x16, [x16]
;   add x16, x16, #32
;   subs xzr, sp, x16, UXTX
;   b.hs 8 ; udf
;   sub sp, sp, #32
; block0:
;   add sp, sp, #32
;   ldp fp, lr, [sp], #16
;   ret

