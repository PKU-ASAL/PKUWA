test compile precise-output
set enable_llvm_abi_extensions=true
target x86_64

function %f0(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = iadd v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   addq    %rdi, %rdx, %rdi
;   adcq    %rsi, %rcx, %rsi
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f1(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = isub v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   subq    %rdi, %rdx, %rdi
;   sbbq    %rsi, %rcx, %rsi
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f2(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = band v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   andq    %rdi, %rdx, %rdi
;   andq    %rsi, %rcx, %rsi
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f3(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   orq     %rdi, %rdx, %rdi
;   orq     %rsi, %rcx, %rsi
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f4(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bxor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   xorq    %rdi, %rdx, %rdi
;   xorq    %rsi, %rcx, %rsi
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f5(i128) -> i128 {
block0(v0: i128):
    v1 = bnot v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   notq    %rdi, %rdi
;   notq    %rsi, %rsi
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f6(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = imul v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %r8
;   imulq   %r8, %rcx, %r8
;   imulq   %rsi, %rdx, %rsi
;   movq    %r8, %r9
;   addq    %r9, %rsi, %r9
;   movq    %r9, %r8
;   movq    %rdi, %rax
;   mul     %rax, %rdx, %rax, %rdx
;   movq    %r8, %rdi
;   addq    %rdi, %rdx, %rdi
;   movq    %rdi, %r8
;   movq    %r8, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f7(i64, i64) -> i128 {
block0(v0: i64, v1: i64):
    v2 = iconcat.i64 v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f8(i128) -> i64, i64 {
block0(v0: i128):
    v1, v2 = isplit.i128 v0
    return v1, v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f9(i128, i128) -> b1 {
block0(v0: i128, v1: i128):
    v2 = icmp eq v0, v1
    v3 = icmp ne v0, v1
    v4 = icmp slt v0, v1
    v5 = icmp sle v0, v1
    v6 = icmp sgt v0, v1
    v7 = icmp sge v0, v1
    v8 = icmp ult v0, v1
    v9 = icmp ule v0, v1
    v10 = icmp ugt v0, v1
    v11 = icmp uge v0, v1
    v12 = band v2, v3
    v13 = band v4, v5
    v14 = band v6, v7
    v15 = band v8, v9
    v16 = band v10, v11
    v17 = band v12, v13
    v18 = band v14, v15
    v19 = band v17, v18
    v20 = band v19, v16
    return v20
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $64, %rsp
;   movq    %rbx, 16(%rsp)
;   movq    %r12, 24(%rsp)
;   movq    %r13, 32(%rsp)
;   movq    %r14, 40(%rsp)
;   movq    %r15, 48(%rsp)
; block0:
;   cmpq    %rdx, %rdi
;   setz    %r10b
;   cmpq    %rcx, %rsi
;   setz    %r11b
;   andq    %r10, %r11, %r10
;   testq   $1, %r10
;   setnz   %al
;   cmpq    %rdx, %rdi
;   setnz   %r8b
;   cmpq    %rcx, %rsi
;   setnz   %r9b
;   orq     %r8, %r9, %r8
;   testq   $1, %r8
;   setnz   %r10b
;   movq    %r10, rsp(0 + virtual offset)
;   cmpq    %rcx, %rsi
;   setl    %r8b
;   setz    %r9b
;   cmpq    %rdx, %rdi
;   setb    %r11b
;   andq    %r9, %r11, %r9
;   orq     %r8, %r9, %r8
;   testq   $1, %r8
;   setnz   %r11b
;   cmpq    %rcx, %rsi
;   setl    %r8b
;   setz    %r9b
;   cmpq    %rdx, %rdi
;   setbe   %bl
;   andq    %r9, %rbx, %r9
;   orq     %r8, %r9, %r8
;   testq   $1, %r8
;   setnz   %r9b
;   cmpq    %rcx, %rsi
;   setnle  %r8b
;   setz    %r13b
;   cmpq    %rdx, %rdi
;   setnbe  %r14b
;   andq    %r13, %r14, %r13
;   orq     %r8, %r13, %r8
;   testq   $1, %r8
;   setnz   %r8b
;   cmpq    %rcx, %rsi
;   setnle  %bl
;   setz    %r12b
;   cmpq    %rdx, %rdi
;   setnb   %r13b
;   andq    %r12, %r13, %r12
;   orq     %rbx, %r12, %rbx
;   testq   $1, %rbx
;   setnz   %r14b
;   cmpq    %rcx, %rsi
;   setb    %r15b
;   setz    %bl
;   cmpq    %rdx, %rdi
;   setb    %r12b
;   andq    %rbx, %r12, %rbx
;   orq     %r15, %rbx, %r15
;   testq   $1, %r15
;   setnz   %r15b
;   cmpq    %rcx, %rsi
;   setb    %r12b
;   setz    %r13b
;   cmpq    %rdx, %rdi
;   setbe   %bl
;   andq    %r13, %rbx, %r13
;   orq     %r12, %r13, %r12
;   testq   $1, %r12
;   setnz   %bl
;   cmpq    %rcx, %rsi
;   setnbe  %r12b
;   setz    %r13b
;   cmpq    %rdx, %rdi
;   setnbe  %r10b
;   andq    %r13, %r10, %r13
;   orq     %r12, %r13, %r12
;   testq   $1, %r12
;   setnz   %r12b
;   cmpq    %rcx, %rsi
;   setnbe  %sil
;   setz    %cl
;   cmpq    %rdx, %rdi
;   setnb   %dil
;   andq    %rcx, %rdi, %rcx
;   orq     %rsi, %rcx, %rsi
;   testq   $1, %rsi
;   setnz   %sil
;   movq    rsp(0 + virtual offset), %rdx
;   andl    %eax, %edx, %eax
;   andl    %r11d, %r9d, %r11d
;   andl    %r8d, %r14d, %r8d
;   andl    %r15d, %ebx, %r15d
;   andl    %r12d, %esi, %r12d
;   andl    %eax, %r11d, %eax
;   andl    %r8d, %r15d, %r8d
;   andl    %eax, %r8d, %eax
;   andl    %eax, %r12d, %eax
;   movq    16(%rsp), %rbx
;   movq    24(%rsp), %r12
;   movq    32(%rsp), %r13
;   movq    40(%rsp), %r14
;   movq    48(%rsp), %r15
;   addq    %rsp, $64, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f10(i128) -> i32 {
block0(v0: i128):
    brz v0, block1
    jump block2

block1:
    v1 = iconst.i32 1
    return v1

block2:
    v2 = iconst.i32 2
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   cmpq    $0, %rdi
;   setz    %r10b
;   cmpq    $0, %rsi
;   setz    %dil
;   testb   %r10b, %dil
;   jnz     label1; j label2
; block1:
;   movl    $1, %eax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
; block2:
;   movl    $2, %eax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f11(i128) -> i32 {
block0(v0: i128):
    brnz v0, block1
    jump block2

block1:
    v1 = iconst.i32 1
    return v1

block2:
    v2 = iconst.i32 2
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   cmpq    $0, %rdi
;   setz    %r10b
;   cmpq    $0, %rsi
;   setz    %dil
;   testb   %r10b, %dil
;   jz      label1; j label2
; block1:
;   movl    $1, %eax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
; block2:
;   movl    $2, %eax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f12(i64) -> i128 {
block0(v0: i64):
    v1 = uextend.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   xorq    %rdx, %rdx, %rdx
;   movq    %rdi, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f13(i64) -> i128 {
block0(v0: i64):
    v1 = sextend.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rdx
;   sarq    $63, %rdx, %rdx
;   movq    %rdi, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f14(i8) -> i128 {
block0(v0: i8):
    v1 = sextend.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movsbq  %dil, %rax
;   movq    %rax, %rdx
;   sarq    $63, %rdx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f15(i8) -> i128 {
block0(v0: i8):
    v1 = uextend.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movzbq  %dil, %rax
;   xorq    %rdx, %rdx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f16(i128) -> i64 {
block0(v0: i128):
    v1 = ireduce.i64 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f17(i128) -> i8 {
block0(v0: i128):
    v1 = ireduce.i8 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f18(b1) -> i128 {
block0(v0: b1):
    v1 = bint.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   andq    %rdi, $1, %rdi
;   xorq    %rdx, %rdx, %rdx
;   movq    %rdi, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f19(i128) -> i128 {
block0(v0: i128):
    v1 = popcnt.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rdx
;   shrq    $1, %rdx, %rdx
;   movabsq $8608480567731124087, %r10
;   andq    %rdx, %r10, %rdx
;   subq    %rdi, %rdx, %rdi
;   shrq    $1, %rdx, %rdx
;   andq    %rdx, %r10, %rdx
;   subq    %rdi, %rdx, %rdi
;   shrq    $1, %rdx, %rdx
;   andq    %rdx, %r10, %rdx
;   subq    %rdi, %rdx, %rdi
;   movq    %rdi, %rax
;   shrq    $4, %rax, %rax
;   addq    %rax, %rdi, %rax
;   movabsq $1085102592571150095, %rcx
;   andq    %rax, %rcx, %rax
;   movabsq $72340172838076673, %r9
;   imulq   %rax, %r9, %rax
;   shrq    $56, %rax, %rax
;   movq    %rsi, %rcx
;   shrq    $1, %rcx, %rcx
;   movabsq $8608480567731124087, %r8
;   andq    %rcx, %r8, %rcx
;   subq    %rsi, %rcx, %rsi
;   shrq    $1, %rcx, %rcx
;   andq    %rcx, %r8, %rcx
;   subq    %rsi, %rcx, %rsi
;   shrq    $1, %rcx, %rcx
;   andq    %rcx, %r8, %rcx
;   subq    %rsi, %rcx, %rsi
;   movq    %rsi, %rcx
;   shrq    $4, %rcx, %rcx
;   addq    %rcx, %rsi, %rcx
;   movabsq $1085102592571150095, %rsi
;   andq    %rcx, %rsi, %rcx
;   movabsq $72340172838076673, %rdx
;   imulq   %rcx, %rdx, %rcx
;   shrq    $56, %rcx, %rcx
;   addq    %rax, %rcx, %rax
;   xorq    %rdx, %rdx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f20(i128) -> i128 {
block0(v0: i128):
    v1 = bitrev.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movabsq $6148914691236517205, %r8
;   movq    %rsi, %r9
;   andq    %r9, %r8, %r9
;   shrq    $1, %rsi, %rsi
;   andq    %rsi, %r8, %rsi
;   shlq    $1, %r9, %r9
;   orq     %r9, %rsi, %r9
;   movabsq $3689348814741910323, %r11
;   movq    %r9, %rsi
;   andq    %rsi, %r11, %rsi
;   shrq    $2, %r9, %r9
;   andq    %r9, %r11, %r9
;   shlq    $2, %rsi, %rsi
;   orq     %rsi, %r9, %rsi
;   movabsq $1085102592571150095, %rax
;   movq    %rsi, %rcx
;   andq    %rcx, %rax, %rcx
;   shrq    $4, %rsi, %rsi
;   andq    %rsi, %rax, %rsi
;   shlq    $4, %rcx, %rcx
;   orq     %rcx, %rsi, %rcx
;   movabsq $71777214294589695, %r8
;   movq    %rcx, %r9
;   andq    %r9, %r8, %r9
;   shrq    $8, %rcx, %rcx
;   andq    %rcx, %r8, %rcx
;   shlq    $8, %r9, %r9
;   orq     %r9, %rcx, %r9
;   movabsq $281470681808895, %rsi
;   movq    %r9, %r11
;   andq    %r11, %rsi, %r11
;   shrq    $16, %r9, %r9
;   andq    %r9, %rsi, %r9
;   shlq    $16, %r11, %r11
;   orq     %r11, %r9, %r11
;   movabsq $4294967295, %rcx
;   movq    %r11, %rax
;   andq    %rax, %rcx, %rax
;   shrq    $32, %r11, %r11
;   shlq    $32, %rax, %rax
;   orq     %rax, %r11, %rax
;   movabsq $6148914691236517205, %rcx
;   movq    %rdi, %rdx
;   andq    %rdx, %rcx, %rdx
;   shrq    $1, %rdi, %rdi
;   andq    %rdi, %rcx, %rdi
;   shlq    $1, %rdx, %rdx
;   orq     %rdx, %rdi, %rdx
;   movabsq $3689348814741910323, %r9
;   movq    %rdx, %r10
;   andq    %r10, %r9, %r10
;   shrq    $2, %rdx, %rdx
;   andq    %rdx, %r9, %rdx
;   shlq    $2, %r10, %r10
;   orq     %r10, %rdx, %r10
;   movabsq $1085102592571150095, %rsi
;   movq    %r10, %rdi
;   andq    %rdi, %rsi, %rdi
;   shrq    $4, %r10, %r10
;   andq    %r10, %rsi, %r10
;   shlq    $4, %rdi, %rdi
;   orq     %rdi, %r10, %rdi
;   movabsq $71777214294589695, %rcx
;   movq    %rdi, %rdx
;   andq    %rdx, %rcx, %rdx
;   shrq    $8, %rdi, %rdi
;   andq    %rdi, %rcx, %rdi
;   shlq    $8, %rdx, %rdx
;   orq     %rdx, %rdi, %rdx
;   movabsq $281470681808895, %r10
;   movq    %rdx, %r9
;   andq    %r9, %r10, %r9
;   shrq    $16, %rdx, %rdx
;   andq    %rdx, %r10, %rdx
;   shlq    $16, %r9, %r9
;   orq     %r9, %rdx, %r9
;   movabsq $4294967295, %rsi
;   movq    %r9, %rdx
;   andq    %rdx, %rsi, %rdx
;   shrq    $32, %r9, %r9
;   shlq    $32, %rdx, %rdx
;   orq     %rdx, %r9, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f21(i128, i64) {
block0(v0: i128, v1: i64):
    store.i128 v0, v1
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, 0(%rdx)
;   movq    %rsi, 8(%rdx)
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f22(i64) -> i128 {
block0(v0: i64):
    v1 = load.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    0(%rdi), %rax
;   movq    8(%rdi), %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f23(i128, b1) -> i128 {
block0(v0: i128, v1: b1):
    v2 = iconst.i128 0
    brnz v1, block1(v2)
    jump block2(v2)

block1(v3: i128):
    v4 = iconst.i128 1
    v5 = iadd.i128 v3, v4
    return v5

block2(v6: i128):
    v7 = iconst.i128 2
    v8 = iadd.i128 v6, v7
    return v8
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   testb   $1, %dl
;   jnz     label1; j label2
; block1:
;   xorq    %rax, %rax, %rax
;   xorq    %rdx, %rdx, %rdx
;   movl    $1, %esi
;   xorq    %rcx, %rcx, %rcx
;   addq    %rax, %rsi, %rax
;   adcq    %rdx, %rcx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
; block2:
;   xorq    %rax, %rax, %rax
;   xorq    %rdx, %rdx, %rdx
;   movl    $2, %r8d
;   xorq    %r10, %r10, %r10
;   addq    %rax, %r8, %rax
;   adcq    %rdx, %r10, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f24(i128, i128, i64, i128, i128, i128) -> i128 {

block0(v0: i128, v1: i128, v2: i64, v3: i128, v4: i128, v5: i128):
    v6 = iadd.i128 v0, v1
    v7 = uextend.i128 v2
    v8 = iadd.i128 v3, v7
    v9 = iadd.i128 v4, v5
    v10 = iadd.i128 v6, v8
    v11 = iadd.i128 v9, v10
    return v11
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %rbx, 0(%rsp)
;   movq    %r13, 8(%rsp)
; block0:
;   movq    16(%rbp), %rbx
;   movq    24(%rbp), %rax
;   movq    32(%rbp), %r10
;   movq    %r10, %r13
;   movq    40(%rbp), %r11
;   movq    48(%rbp), %r10
;   addq    %rdi, %rdx, %rdi
;   movq    %rcx, %rdx
;   adcq    %rsi, %rdx, %rsi
;   xorq    %rdx, %rdx, %rdx
;   addq    %r9, %r8, %r9
;   adcq    %rbx, %rdx, %rbx
;   addq    %rax, %r11, %rax
;   movq    %r13, %rdx
;   adcq    %rdx, %r10, %rdx
;   addq    %rdi, %r9, %rdi
;   adcq    %rsi, %rbx, %rsi
;   addq    %rax, %rdi, %rax
;   adcq    %rdx, %rsi, %rdx
;   movq    0(%rsp), %rbx
;   movq    8(%rsp), %r13
;   addq    %rsp, $16, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f25(i128) -> i128, i128, i128, i64, i128, i128 {
block0(v0: i128):
    v1 = ireduce.i64 v0
    return v0, v0, v0, v1, v0, v0
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $32, %rsp
;   movq    %rbx, 0(%rsp)
;   movq    %r12, 8(%rsp)
;   movq    %r14, 16(%rsp)
;   movq    %r15, 24(%rsp)
; block0:
;   movq    %rdx, %r12
;   movq    %rdi, %rax
;   movq    %rsi, %rdx
;   movq    %rdi, %r14
;   movq    %rsi, %rbx
;   movq    %rdi, %r11
;   movq    %rsi, %r9
;   movq    %rdi, %r10
;   movq    %rdi, %r8
;   movq    %rsi, %rcx
;   movq    %r12, %r15
;   movq    %r14, 0(%r15)
;   movq    %rbx, 8(%r15)
;   movq    %r11, 16(%r15)
;   movq    %r9, 24(%r15)
;   movq    %r10, 32(%r15)
;   movq    %r8, 40(%r15)
;   movq    %rcx, 48(%r15)
;   movq    %rdi, 56(%r15)
;   movq    %rsi, 64(%r15)
;   movq    0(%rsp), %rbx
;   movq    8(%rsp), %r12
;   movq    16(%rsp), %r14
;   movq    24(%rsp), %r15
;   addq    %rsp, $32, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f26(i128, i128) -> i128, i128 {
    fn0 = %g(i128, i128) -> i128, i128
block0(v0: i128, v1: i128):
    v2, v3 = call fn0(v0, v1)
    return v2, v3
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %r12, 0(%rsp)
; block0:
;   movq    %r8, %r12
;   subq    %rsp, $16, %rsp
;   virtual_sp_offset_adjust 16
;   lea     0(%rsp), %r8
;   load_ext_name %g+0, %rax
;   call    *%rax
;   movq    0(%rsp), %r11
;   movq    8(%rsp), %rdi
;   addq    %rsp, $16, %rsp
;   virtual_sp_offset_adjust -16
;   movq    %r12, %r8
;   movq    %r11, 0(%r8)
;   movq    %rdi, 8(%r8)
;   movq    0(%rsp), %r12
;   addq    %rsp, $16, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f27(i128) -> i128 {
block0(v0: i128):
    v1 = clz.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movabsq $-1, %r8
;   bsrq    %rsi, %r11
;   cmovzq  %r8, %r11, %r11
;   movl    $63, %ecx
;   subq    %rcx, %r11, %rcx
;   movabsq $-1, %r9
;   bsrq    %rdi, %rsi
;   cmovzq  %r9, %rsi, %rsi
;   movl    $63, %eax
;   subq    %rax, %rsi, %rax
;   addq    %rax, $64, %rax
;   cmpq    $64, %rcx
;   cmovnzq %rcx, %rax, %rax
;   xorq    %rdx, %rdx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f28(i128) -> i128 {
block0(v0: i128):
    v1 = ctz.i128 v0
    return v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movl    $64, %r8d
;   bsfq    %rdi, %rax
;   cmovzq  %r8, %rax, %rax
;   movl    $64, %ecx
;   bsfq    %rsi, %r9
;   cmovzq  %rcx, %r9, %r9
;   addq    %r9, $64, %r9
;   cmpq    $64, %rax
;   cmovzq  %r9, %rax, %rax
;   xorq    %rdx, %rdx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f29(i8, i128) -> i8 {
block0(v0: i8, v1: i128):
    v2 = ishl v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rsi, %rcx
;   andq    %rcx, $7, %rcx
;   shlb    %cl, %dil, %dil
;   movq    %rdi, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f30(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = ishl v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdx, %rcx
;   movq    %rdi, %rdx
;   shlq    %cl, %rdx, %rdx
;   shlq    %cl, %rsi, %rsi
;   movq    %rcx, %rax
;   movl    $64, %ecx
;   movq    %rax, %r8
;   subq    %rcx, %r8, %rcx
;   shrq    %cl, %rdi, %rdi
;   xorq    %rax, %rax, %rax
;   testq   $127, %r8
;   cmovzq  %rax, %rdi, %rdi
;   orq     %rdi, %rsi, %rdi
;   testq   $64, %r8
;   cmovzq  %rdx, %rax, %rax
;   cmovzq  %rdi, %rdx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f31(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = ushr v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdx, %rcx
;   shrq    %cl, %rdi, %rdi
;   movq    %rsi, %r8
;   shrq    %cl, %r8, %r8
;   movl    $64, %ecx
;   movq    %rdx, %r9
;   subq    %rcx, %r9, %rcx
;   shlq    %cl, %rsi, %rsi
;   xorq    %r11, %r11, %r11
;   testq   $127, %r9
;   cmovzq  %r11, %rsi, %rsi
;   orq     %rsi, %rdi, %rsi
;   xorq    %rdx, %rdx, %rdx
;   testq   $64, %r9
;   movq    %r8, %rax
;   cmovzq  %rsi, %rax, %rax
;   cmovzq  %r8, %rdx, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f32(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = sshr v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdx, %rcx
;   shrq    %cl, %rdi, %rdi
;   movq    %rsi, %rdx
;   sarq    %cl, %rdx, %rdx
;   movq    %rcx, %rax
;   movl    $64, %ecx
;   movq    %rax, %r8
;   subq    %rcx, %r8, %rcx
;   movq    %rsi, %r11
;   shlq    %cl, %r11, %r11
;   xorq    %rax, %rax, %rax
;   testq   $127, %r8
;   cmovzq  %rax, %r11, %r11
;   orq     %rdi, %r11, %rdi
;   sarq    $63, %rsi, %rsi
;   testq   $64, %r8
;   movq    %rdx, %rax
;   cmovzq  %rdi, %rax, %rax
;   cmovzq  %rdx, %rsi, %rsi
;   movq    %rsi, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f33(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = rotl v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdx, %rcx
;   movq    %rdi, %rdx
;   shlq    %cl, %rdx, %rdx
;   movq    %rsi, %r9
;   shlq    %cl, %r9, %r9
;   movq    %rcx, %r10
;   movl    $64, %ecx
;   subq    %rcx, %r10, %rcx
;   movq    %rdi, %r8
;   shrq    %cl, %r8, %r8
;   xorq    %rax, %rax, %rax
;   testq   $127, %r10
;   cmovzq  %rax, %r8, %r8
;   orq     %r8, %r9, %r8
;   testq   $64, %r10
;   cmovzq  %rdx, %rax, %rax
;   cmovzq  %r8, %rdx, %rdx
;   movl    $128, %ecx
;   movq    %r10, %r8
;   subq    %rcx, %r8, %rcx
;   shrq    %cl, %rdi, %rdi
;   movq    %rsi, %r11
;   shrq    %cl, %r11, %r11
;   movq    %rcx, %r8
;   movl    $64, %ecx
;   subq    %rcx, %r8, %rcx
;   shlq    %cl, %rsi, %rsi
;   xorq    %r10, %r10, %r10
;   testq   $127, %r8
;   cmovzq  %r10, %rsi, %rsi
;   orq     %rsi, %rdi, %rsi
;   xorq    %r10, %r10, %r10
;   testq   $64, %r8
;   movq    %r11, %rdi
;   cmovzq  %rsi, %rdi, %rdi
;   cmovzq  %r11, %r10, %r10
;   orq     %rax, %rdi, %rax
;   orq     %rdx, %r10, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f34(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = rotr v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdx, %rcx
;   movq    %rdi, %r10
;   shrq    %cl, %r10, %r10
;   movq    %rsi, %r8
;   shrq    %cl, %r8, %r8
;   movq    %rcx, %r11
;   movl    $64, %ecx
;   movq    %r11, %rax
;   subq    %rcx, %rax, %rcx
;   movq    %rsi, %r9
;   shlq    %cl, %r9, %r9
;   xorq    %r11, %r11, %r11
;   testq   $127, %rax
;   cmovzq  %r11, %r9, %r9
;   orq     %r9, %r10, %r9
;   xorq    %rdx, %rdx, %rdx
;   testq   $64, %rax
;   movq    %rax, %r11
;   movq    %r8, %rax
;   cmovzq  %r9, %rax, %rax
;   cmovzq  %r8, %rdx, %rdx
;   movl    $128, %ecx
;   movq    %r11, %r8
;   subq    %rcx, %r8, %rcx
;   movq    %rdi, %r11
;   shlq    %cl, %r11, %r11
;   shlq    %cl, %rsi, %rsi
;   movq    %rcx, %r8
;   movl    $64, %ecx
;   movq    %r8, %r9
;   subq    %rcx, %r9, %rcx
;   shrq    %cl, %rdi, %rdi
;   xorq    %r8, %r8, %r8
;   testq   $127, %r9
;   cmovzq  %r8, %rdi, %rdi
;   orq     %rdi, %rsi, %rdi
;   testq   $64, %r9
;   cmovzq  %r11, %r8, %r8
;   cmovzq  %rdi, %r11, %r11
;   orq     %rax, %r8, %rax
;   orq     %rdx, %r11, %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

