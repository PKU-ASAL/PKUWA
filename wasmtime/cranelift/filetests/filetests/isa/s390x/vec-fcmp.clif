test compile precise-output
target s390x

function %fcmp_eq_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 eq v0, v1
  return v2
}

; block0:
;   vfcedb %v24, %v24, %v25
;   br %r14

function %fcmp_ne_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 ne v0, v1
  return v2
}

; block0:
;   vfcedb %v4, %v24, %v25
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_gt_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 gt v0, v1
  return v2
}

; block0:
;   vfchdb %v24, %v24, %v25
;   br %r14

function %fcmp_lt_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 lt v0, v1
  return v2
}

; block0:
;   vfchdb %v24, %v25, %v24
;   br %r14

function %fcmp_ge_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 ge v0, v1
  return v2
}

; block0:
;   vfchedb %v24, %v24, %v25
;   br %r14

function %fcmp_le_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 le v0, v1
  return v2
}

; block0:
;   vfchedb %v24, %v25, %v24
;   br %r14

function %fcmp_ueq_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 ueq v0, v1
  return v2
}

; block0:
;   vfchdb %v4, %v24, %v25
;   vfchdb %v6, %v25, %v24
;   vno %v24, %v4, %v6
;   br %r14

function %fcmp_one_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 one v0, v1
  return v2
}

; block0:
;   vfchdb %v4, %v24, %v25
;   vfchdb %v6, %v25, %v24
;   vo %v24, %v4, %v6
;   br %r14

function %fcmp_ugt_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 ugt v0, v1
  return v2
}

; block0:
;   vfchedb %v4, %v25, %v24
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_ult_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 ult v0, v1
  return v2
}

; block0:
;   vfchedb %v4, %v24, %v25
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_uge_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 uge v0, v1
  return v2
}

; block0:
;   vfchdb %v4, %v25, %v24
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_ule_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 ule v0, v1
  return v2
}

; block0:
;   vfchdb %v4, %v24, %v25
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_ord_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 ord v0, v1
  return v2
}

; block0:
;   vfchedb %v4, %v24, %v25
;   vfchedb %v6, %v25, %v24
;   vo %v24, %v4, %v6
;   br %r14

function %fcmp_uno_f64x2(f64x2, f64x2) -> b64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp.f64x2 uno v0, v1
  return v2
}

; block0:
;   vfchedb %v4, %v24, %v25
;   vfchedb %v6, %v25, %v24
;   vno %v24, %v4, %v6
;   br %r14

function %fcmp_eq_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 eq v0, v1
  return v2
}

; block0:
;   vfcesb %v24, %v24, %v25
;   br %r14

function %fcmp_ne_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 ne v0, v1
  return v2
}

; block0:
;   vfcesb %v4, %v24, %v25
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_gt_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 gt v0, v1
  return v2
}

; block0:
;   vfchsb %v24, %v24, %v25
;   br %r14

function %fcmp_lt_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 lt v0, v1
  return v2
}

; block0:
;   vfchsb %v24, %v25, %v24
;   br %r14

function %fcmp_ge_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 ge v0, v1
  return v2
}

; block0:
;   vfchesb %v24, %v24, %v25
;   br %r14

function %fcmp_le_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 le v0, v1
  return v2
}

; block0:
;   vfchesb %v24, %v25, %v24
;   br %r14

function %fcmp_ueq_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 ueq v0, v1
  return v2
}

; block0:
;   vfchsb %v4, %v24, %v25
;   vfchsb %v6, %v25, %v24
;   vno %v24, %v4, %v6
;   br %r14

function %fcmp_one_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 one v0, v1
  return v2
}

; block0:
;   vfchsb %v4, %v24, %v25
;   vfchsb %v6, %v25, %v24
;   vo %v24, %v4, %v6
;   br %r14

function %fcmp_ugt_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 ugt v0, v1
  return v2
}

; block0:
;   vfchesb %v4, %v25, %v24
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_ult_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 ult v0, v1
  return v2
}

; block0:
;   vfchesb %v4, %v24, %v25
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_uge_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 uge v0, v1
  return v2
}

; block0:
;   vfchsb %v4, %v25, %v24
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_ule_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 ule v0, v1
  return v2
}

; block0:
;   vfchsb %v4, %v24, %v25
;   vno %v24, %v4, %v4
;   br %r14

function %fcmp_ord_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 ord v0, v1
  return v2
}

; block0:
;   vfchesb %v4, %v24, %v25
;   vfchesb %v6, %v25, %v24
;   vo %v24, %v4, %v6
;   br %r14

function %fcmp_uno_f32x4(f32x4, f32x4) -> b32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp.f32x4 uno v0, v1
  return v2
}

; block0:
;   vfchesb %v4, %v24, %v25
;   vfchesb %v6, %v25, %v24
;   vno %v24, %v4, %v6
;   br %r14

