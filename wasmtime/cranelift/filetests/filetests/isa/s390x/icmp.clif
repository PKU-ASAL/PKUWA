test compile precise-output
target s390x

function %icmp_slt_i64(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = icmp.i64 slt v0, v1
  return v2
}

; block0:
;   cgr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_ext32(i64, i32) -> b1 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = icmp.i64 slt v0, v2
  return v3
}

; block0:
;   cgfr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_imm16(i64) -> b1 {
block0(v0: i64):
  v1 = iconst.i64 1
  v2 = icmp.i64 slt v0, v1
  return v2
}

; block0:
;   cghi %r2, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_imm32(i64) -> b1 {
block0(v0: i64):
  v1 = iconst.i64 32768
  v2 = icmp.i64 slt v0, v1
  return v2
}

; block0:
;   cgfi %r2, 32768
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_mem(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = load.i64 v1
  v3 = icmp.i64 slt v0, v2
  return v3
}

; block0:
;   cg %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_sym(i64) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i64):
  v1 = symbol_value.i64 gv0
  v2 = load.i64 aligned v1
  v3 = icmp.i64 slt v0, v2
  return v3
}

; block0:
;   cgrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_mem_ext16(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = sload16.i64 v1
  v3 = icmp.i64 slt v0, v2
  return v3
}

; block0:
;   cgh %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_sym_ext16(i64) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i64):
  v1 = symbol_value.i64 gv0
  v2 = sload16.i64 aligned v1
  v3 = icmp.i64 slt v0, v2
  return v3
}

; block0:
;   cghrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_mem_ext32(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = sload32.i64 v1
  v3 = icmp.i64 slt v0, v2
  return v3
}

; block0:
;   cgf %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i64_sym_ext32(i64) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i64):
  v1 = symbol_value.i64 gv0
  v2 = sload32.i64 aligned v1
  v3 = icmp.i64 slt v0, v2
  return v3
}

; block0:
;   cgfrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32(i32, i32) -> b1 {
block0(v0: i32, v1: i32):
  v2 = icmp.i32 slt v0, v1
  return v2
}

; block0:
;   cr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_imm16(i32) -> b1 {
block0(v0: i32):
  v1 = iconst.i32 1
  v2 = icmp.i32 slt v0, v1
  return v2
}

; block0:
;   chi %r2, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_imm(i32) -> b1 {
block0(v0: i32):
  v1 = iconst.i32 32768
  v2 = icmp.i32 slt v0, v1
  return v2
}

; block0:
;   cfi %r2, 32768
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_mem(i32, i64) -> b1 {
block0(v0: i32, v1: i64):
  v2 = load.i32 v1
  v3 = icmp.i32 slt v0, v2
  return v3
}

; block0:
;   c %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_memoff(i32, i64) -> b1 {
block0(v0: i32, v1: i64):
  v2 = load.i32 v1+4096
  v3 = icmp.i32 slt v0, v2
  return v3
}

; block0:
;   cy %r2, 4096(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_sym(i32) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i32):
  v1 = symbol_value.i64 gv0
  v2 = load.i32 aligned v1
  v3 = icmp.i32 slt v0, v2
  return v3
}

; block0:
;   crl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_mem_ext16(i32, i64) -> b1 {
block0(v0: i32, v1: i64):
  v2 = sload16.i32 v1
  v3 = icmp.i32 slt v0, v2
  return v3
}

; block0:
;   ch %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_memoff_ext16(i32, i64) -> b1 {
block0(v0: i32, v1: i64):
  v2 = sload16.i32 v1+4096
  v3 = icmp.i32 slt v0, v2
  return v3
}

; block0:
;   chy %r2, 4096(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i32_sym_ext16(i32) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i32):
  v1 = symbol_value.i64 gv0
  v2 = sload16.i32 aligned v1
  v3 = icmp.i32 slt v0, v2
  return v3
}

; block0:
;   chrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i16(i16, i16) -> b1 {
block0(v0: i16, v1: i16):
  v2 = icmp.i16 slt v0, v1
  return v2
}

; block0:
;   lhr %r2, %r2
;   lhr %r4, %r3
;   cr %r2, %r4
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i16_imm(i16) -> b1 {
block0(v0: i16):
  v1 = iconst.i16 1
  v2 = icmp.i16 slt v0, v1
  return v2
}

; block0:
;   lhr %r5, %r2
;   chi %r5, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i16_mem(i16, i64) -> b1 {
block0(v0: i16, v1: i64):
  v2 = load.i16 v1
  v3 = icmp.i16 slt v0, v2
  return v3
}

; block0:
;   lhr %r2, %r2
;   ch %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i16_sym(i16) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i16):
  v1 = symbol_value.i64 gv0
  v2 = load.i16 aligned v1
  v3 = icmp.i16 slt v0, v2
  return v3
}

; block0:
;   lhr %r5, %r2
;   chrl %r5, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i8(i8, i8) -> b1 {
block0(v0: i8, v1: i8):
  v2 = icmp.i8 slt v0, v1
  return v2
}

; block0:
;   lbr %r2, %r2
;   lbr %r4, %r3
;   cr %r2, %r4
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i8_imm(i8) -> b1 {
block0(v0: i8):
  v1 = iconst.i8 1
  v2 = icmp.i8 slt v0, v1
  return v2
}

; block0:
;   lbr %r5, %r2
;   chi %r5, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_slt_i8_mem(i8, i64) -> b1 {
block0(v0: i8, v1: i64):
  v2 = load.i8 v1
  v3 = icmp.i8 slt v0, v2
  return v3
}

; block0:
;   lbr %r2, %r2
;   lb %r4, 0(%r3)
;   cr %r2, %r4
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = icmp.i64 ult v0, v1
  return v2
}

; block0:
;   clgr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_ext32(i64, i32) -> b1 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = icmp.i64 ult v0, v2
  return v3
}

; block0:
;   clgfr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_imm(i64) -> b1 {
block0(v0: i64):
  v1 = iconst.i64 1
  v2 = icmp.i64 ult v0, v1
  return v2
}

; block0:
;   clgfi %r2, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_mem(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = load.i64 v1
  v3 = icmp.i64 ult v0, v2
  return v3
}

; block0:
;   clg %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_sym(i64) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i64):
  v1 = symbol_value.i64 gv0
  v2 = load.i64 aligned v1
  v3 = icmp.i64 ult v0, v2
  return v3
}

; block0:
;   clgrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_mem_ext32(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = uload32.i64 v1
  v3 = icmp.i64 ult v0, v2
  return v3
}

; block0:
;   clgf %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_sym_ext32(i64) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i64):
  v1 = symbol_value.i64 gv0
  v2 = uload32.i64 aligned v1
  v3 = icmp.i64 ult v0, v2
  return v3
}

; block0:
;   clgfrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_mem_ext16(i64, i64) -> b1 {
block0(v0: i64, v1: i64):
  v2 = uload16.i64 v1
  v3 = icmp.i64 ult v0, v2
  return v3
}

; block0:
;   llgh %r3, 0(%r3)
;   clgr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i64_sym_ext16(i64) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i64):
  v1 = symbol_value.i64 gv0
  v2 = uload16.i64 aligned v1
  v3 = icmp.i64 ult v0, v2
  return v3
}

; block0:
;   clghrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i32(i32, i32) -> b1 {
block0(v0: i32, v1: i32):
  v2 = icmp.i32 ult v0, v1
  return v2
}

; block0:
;   clr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i32_imm(i32) -> b1 {
block0(v0: i32):
  v1 = iconst.i32 1
  v2 = icmp.i32 ult v0, v1
  return v2
}

; block0:
;   clfi %r2, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i32_mem(i32, i64) -> b1 {
block0(v0: i32, v1: i64):
  v2 = load.i32 v1
  v3 = icmp.i32 ult v0, v2
  return v3
}

; block0:
;   cl %r2, 0(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i32_memoff(i32, i64) -> b1 {
block0(v0: i32, v1: i64):
  v2 = load.i32 v1+4096
  v3 = icmp.i32 ult v0, v2
  return v3
}

; block0:
;   cly %r2, 4096(%r3)
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i32_sym(i32) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i32):
  v1 = symbol_value.i64 gv0
  v2 = load.i32 aligned v1
  v3 = icmp.i32 ult v0, v2
  return v3
}

; block0:
;   clrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i32_mem_ext16(i32, i64) -> b1 {
block0(v0: i32, v1: i64):
  v2 = uload16.i32 v1
  v3 = icmp.i32 ult v0, v2
  return v3
}

; block0:
;   llh %r3, 0(%r3)
;   clr %r2, %r3
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i32_sym_ext16(i32) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i32):
  v1 = symbol_value.i64 gv0
  v2 = uload16.i32 aligned v1
  v3 = icmp.i32 ult v0, v2
  return v3
}

; block0:
;   clhrl %r2, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i16(i16, i16) -> b1 {
block0(v0: i16, v1: i16):
  v2 = icmp.i16 ult v0, v1
  return v2
}

; block0:
;   llhr %r2, %r2
;   llhr %r4, %r3
;   clr %r2, %r4
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i16_imm(i16) -> b1 {
block0(v0: i16):
  v1 = iconst.i16 1
  v2 = icmp.i16 ult v0, v1
  return v2
}

; block0:
;   llhr %r5, %r2
;   clfi %r5, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i16_mem(i16, i64) -> b1 {
block0(v0: i16, v1: i64):
  v2 = load.i16 v1
  v3 = icmp.i16 ult v0, v2
  return v3
}

; block0:
;   llhr %r2, %r2
;   llh %r4, 0(%r3)
;   clr %r2, %r4
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i16_mem(i16) -> b1 {
  gv0 = symbol colocated %sym
block0(v0: i16):
  v1 = symbol_value.i64 gv0
  v2 = load.i16 aligned v1
  v3 = icmp.i16 ult v0, v2
  return v3
}

; block0:
;   llhr %r5, %r2
;   clhrl %r5, %sym + 0
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i8(i8, i8) -> b1 {
block0(v0: i8, v1: i8):
  v2 = icmp.i8 ult v0, v1
  return v2
}

; block0:
;   llcr %r2, %r2
;   llcr %r4, %r3
;   clr %r2, %r4
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i8_imm(i8) -> b1 {
block0(v0: i8):
  v1 = iconst.i8 1
  v2 = icmp.i8 ult v0, v1
  return v2
}

; block0:
;   llcr %r5, %r2
;   clfi %r5, 1
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

function %icmp_ult_i8_mem(i8, i64) -> b1 {
block0(v0: i8, v1: i64):
  v2 = load.i8 v1
  v3 = icmp.i8 ult v0, v2
  return v3
}

; block0:
;   llcr %r2, %r2
;   llc %r4, 0(%r3)
;   clr %r2, %r4
;   lhi %r2, 0
;   lochil %r2, 1
;   br %r14

