test simple_preopt
target aarch64
target i686 baseline

; -------- U32 --------

; complex case (mul, sub, shift, add, shift)
function %t_urem32_p7(i32) -> i32 {
block0(v0: i32):
    v1 = urem_imm v0, 7
    ; check: iconst.i32 0x2492_4925
    ; check: umulhi v0, v2
    ; check: isub v0, v3
    ; check: ushr_imm v4, 1
    ; check: iadd v5, v3
    ; check: ushr_imm v6, 2
    ; check: imul_imm v7, 7
    ; check: isub v0, v8
    return v1
}

; simple case (mul, shift)
function %t_urem32_p125(i32) -> i32 {
block0(v0: i32):
    v1 = urem_imm v0, 125
    ; check: iconst.i32 0x1062_4dd3
    ; check: umulhi v0, v2
    ; check: ushr_imm v3, 3
    ; check: imul_imm v4, 125
    ; check: isub v0, v5
    return v1
}

; simple case w/ shift by zero (mul)
function %t_urem32_p641(i32) -> i32 {
block0(v0: i32):
    v1 = urem_imm v0, 641
    ; check: iconst.i32 0x0066_3d81
    ; check: umulhi v0, v2
    ; check: imul_imm v3, 641
    ; check: isub v0, v4
    return v1
}


; -------- S32 --------

; simple case w/ shift by zero (mul, add-sign-bit)
function %t_srem32_n6(i32) -> i32 {
block0(v0: i32):
    v1 = srem_imm v0, -6
    ; check: iconst.i32 0xffff_ffff_d555_5555
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 31
    ; check: iadd v3, v4
    ; check: imul_imm v5, -6
    ; check: isub v0, v6
    return v1
}

; simple case (mul, shift, add-sign-bit)
function %t_srem32_n5(i32) -> i32 {
block0(v0: i32):
    v1 = srem_imm v0, -5
    ; check: iconst.i32 0xffff_ffff_9999_9999
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 1
    ; check: ushr_imm v4, 31
    ; check: iadd v4, v5
    ; check: imul_imm v6, -5
    ; check: isub v0, v7
    return v1
}

; case d < 0 && M > 0 (mul, sub, shift, add-sign-bit)
function %t_srem32_n3(i32) -> i32 {
block0(v0: i32):
    v1 = srem_imm v0, -3
    ; check: iconst.i32 0x5555_5555
    ; check: smulhi v0, v2
    ; check: isub v3, v0
    ; check: sshr_imm v4, 1
    ; check: ushr_imm v5, 31
    ; check: iadd v5, v6
    ; check: imul_imm v7, -3
    ; check: isub v0, v8
    return v1
}

; simple case w/ shift by zero (mul, add-sign-bit)
function %t_srem32_p6(i32) -> i32 {
block0(v0: i32):
    v1 = srem_imm v0, 6
    ; check: iconst.i32 0x2aaa_aaab
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 31
    ; check: iadd v3, v4
    ; check: imul_imm v5, 6
    ; check: isub v0, v6
    return v1
}

; case d > 0 && M < 0 (mull, add, shift, add-sign-bit)
function %t_srem32_p7(i32) -> i32 {
block0(v0: i32):
    v1 = srem_imm v0, 7
    ; check: iconst.i32 0xffff_ffff_9249_2493
    ; check: smulhi v0, v2
    ; check: iadd v3, v0
    ; check: sshr_imm v4, 2
    ; check: ushr_imm v5, 31
    ; check: iadd v5, v6
    ; check: imul_imm v7, 7
    ; check: isub v0, v8
    return v1
}

; simple case (mul, shift, add-sign-bit)
function %t_srem32_p625(i32) -> i32 {
block0(v0: i32):
    v1 = srem_imm v0, 625
    ; check: iconst.i32 0x68db_8bad
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 8
    ; check: ushr_imm v4, 31
    ; check: iadd v4, v5
    ; check: imul_imm v6, 625
    ; check: isub v0, v7
    return v1
}


; -------- U64 --------

; complex case (mul, sub, shift, add, shift)
function %t_urem64_p7(i64) -> i64 {
block0(v0: i64):
    v1 = urem_imm v0, 7
    ; check: umulhi v0, v2
    ; check: isub v0, v3
    ; check: ushr_imm v4, 1
    ; check: iadd v5, v3
    ; check: ushr_imm v6, 2
    ; check: imul_imm v7, 7
    ; check: isub v0, v8
    return v1
}

; simple case (mul, shift)
function %t_urem64_p9(i64) -> i64 {
block0(v0: i64):
    v1 = urem_imm v0, 9
    ; check: iconst.i64 0xe38e_38e3_8e38_e38f
    ; check: umulhi v0, v2
    ; check: ushr_imm v3, 3
    ; check: imul_imm v4, 9
    ; check: isub v0, v5
    return v1
}

; complex case (mul, sub, shift, add, shift)
function %t_urem64_p125(i64) -> i64 {
block0(v0: i64):
    v1 = urem_imm v0, 125
    ; check: iconst.i64 0x0624_dd2f_1a9f_be77
    ; check: umulhi v0, v2
    ; check: isub v0, v3
    ; check: ushr_imm v4, 1
    ; check: iadd v5, v3
    ; check: ushr_imm v6, 6
    ; check: imul_imm v7, 125
    ; check: isub v0, v8
    return v1
}

; simple case w/ shift by zero (mul)
function %t_urem64_p274177(i64) -> i64 {
block0(v0: i64):
    v1 = urem_imm v0, 274177
    ; check: iconst.i64 0x3d30_f19c_d101
    ; check: umulhi v0, v2
    ; check: imul_imm v3, 0x0004_2f01
    ; check: isub v0, v4
    return v1
}


; -------- S64 --------

; simple case (mul, shift, add-sign-bit)
function %t_srem64_n625(i64) -> i64 {
block0(v0: i64):
    v1 = srem_imm v0, -625
    ; check: iconst.i64 0xcb92_3a29_c779_a6b5
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 7
    ; check: ushr_imm v4, 63
    ; check: iadd v4, v5
    ; check: imul_imm v6, -625
    ; check: isub v0, v7
    return v1
}

; simple case w/ zero shift (mul, add-sign-bit)
function %t_srem64_n6(i64) -> i64 {
block0(v0: i64):
    v1 = srem_imm v0, -6
    ; check: iconst.i64 0xd555_5555_5555_5555
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 63
    ; check: iadd v3, v4
    ; check: imul_imm v5, -6
    ; check: isub v0, v6
    return v1
}

; simple case w/ zero shift (mul, add-sign-bit)
function %t_srem64_n5(i64) -> i64 {
block0(v0: i64):
    v1 = srem_imm v0, -5
    ; check: iconst.i64 0x9999_9999_9999_9999
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 1
    ; check: ushr_imm v4, 63
    ; check: iadd v4, v5
    ; check: imul_imm v6, -5
    ; check: isub v0, v7
    return v1
}

; case d < 0 && M > 0 (mul, sub, shift, add-sign-bit)
function %t_srem64_n3(i64) -> i64 {
block0(v0: i64):
    v1 = srem_imm v0, -3
    ; check: iconst.i64 0x5555_5555_5555_5555
    ; check: smulhi v0, v2
    ; check: isub v3, v0
    ; check: sshr_imm v4, 1
    ; check: ushr_imm v5, 63
    ; check: iadd v5, v6
    ; check: imul_imm v7, -3
    ; check: isub v0, v8
    return v1
}

; simple case w/ zero shift (mul, add-sign-bit)
function %t_srem64_p6(i64) -> i64 {
block0(v0: i64):
    v1 = srem_imm v0, 6
    ; check: iconst.i64 0x2aaa_aaaa_aaaa_aaab
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 63
    ; check: iadd v3, v4
    ; check: imul_imm v5, 6
    ; check: isub v0, v6
    return v1
}

; case d > 0 && M < 0 (mul, add, shift, add-sign-bit)
function %t_srem64_p15(i64) -> i64 {
block0(v0: i64):
    v1 = srem_imm v0, 15
    ; check: iconst.i64 0x8888_8888_8888_8889
    ; check: smulhi v0, v2
    ; check: iadd v3, v0
    ; check: sshr_imm v4, 3
    ; check: ushr_imm v5, 63
    ; check: iadd v5, v6
    ; check: imul_imm v7, 15
    ; check: isub v0, v8
    return v1
}

; simple case (mul, shift, add-sign-bit)
function %t_srem64_p625(i64) -> i64 {
block0(v0: i64):
    v1 = srem_imm v0, 625
    ; check: iconst.i64 0x346d_c5d6_3886_594b
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 7
    ; check: ushr_imm v4, 63
    ; check: iadd v4, v5
    ; check: imul_imm v6, 625
    ; check: isub v0, v7
    return v1
}
